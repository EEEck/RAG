{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Textbook RAG Demo\n",
    "\n",
    "This notebook demonstrates the complete RAG pipeline for a textbook, including:\n",
    "1. **Ingestion**: Parsing a textbook (PDF/JSON) into structure nodes and content atoms.\n",
    "2. **Ground Truth Extraction**: Extracting vocabulary using the specialized extractor for verification.\n",
    "3. **RAG Query**: Generating a quiz for a specific unit using the RAG engine with safeguards.\n",
    "4. **Verification**: verifying that the generated quiz respects the vocabulary constraints of the target unit.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Postgres database running (with `pgvector` extension).\n",
    "- `OPENAI_API_KEY` environment variable set.\n",
    "- Dependencies installed (`pip install -r requirements.txt`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Ensure project root is in path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
    "\n",
    "from ingest.service import IngestionService\n",
    "from ingest.infra.postgres import PostgresStructureNodeRepository\n",
    "from ingest.hybrid_ingestor import HybridIngestor\n",
    "from ingest.docling_parser import load_docling_blocks\n",
    "from ingest.segmentation import SegmentationRules, segment_lessons\n",
    "from ingest.vocab_extractor import extract_vocab_entries, link_vocab_to_lessons\n",
    "from app.rag_engine import retrieve_and_generate\n",
    "from app.schemas import GenerateItemsRequest, ConceptPack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "FILE_PATH = \"../../data/toy_green_line_1_docling.json\" # Relative path from docs/notebooks/\n",
    "BOOK_ID = uuid.uuid4()\n",
    "UNIT_TO_TEST = 1\n",
    "TOPIC = \"Create a quiz for Unit 1\"\n",
    "CATEGORY = \"language\"\n",
    "\n",
    "# Check for API Key\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"WARNING: OPENAI_API_KEY not set. Please set it to use real embeddings/LLM.\")\n",
    "    should_mock = True\n",
    "else:\n",
    "    should_mock = False\n",
    "    print(\"OPENAI_API_KEY detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Ingestion\n",
    "We use the `IngestionService` to process the book. This parses the content, creates structure nodes in Postgres, and indexes content atoms in the Vector Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"--- Starting Ingestion for Book ID: {BOOK_ID} ---\")\n",
    "\n",
    "# Initialize Components\n",
    "repo = PostgresStructureNodeRepository()\n",
    "ingestor = HybridIngestor()\n",
    "service = IngestionService(\n",
    "    structure_repo=repo,\n",
    "    ingestor=ingestor,\n",
    "    should_mock_embedding=should_mock\n",
    ")\n",
    "\n",
    "# Run Ingestion\n",
    "# Note: In a real notebook, ensure Postgres is running locally or via Docker\n",
    "try:\n",
    "    service.ingest_book(FILE_PATH, book_id=BOOK_ID, category=CATEGORY)\n",
    "except Exception as e:\n",
    "    print(f\"Ingestion failed: {e}\")\n",
    "    print(\"Ensure your Postgres database is running and accessible via env vars (POSTGRES_HOST, etc).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Ground Truth Vocabulary\n",
    "To verify our safeguards, we extract the \"official\" vocabulary list for Unit 1 using the `vocab_extractor` module. This gives us the list of allowed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extracting Ground Truth Vocabulary...\")\n",
    "path = Path(FILE_PATH)\n",
    "blocks = load_docling_blocks(path)\n",
    "rules = SegmentationRules()\n",
    "\n",
    "# Segment into lessons/units\n",
    "lessons = segment_lessons(blocks, rules, textbook_id=str(BOOK_ID))\n",
    "\n",
    "# Extract vocab\n",
    "vocab_entries = extract_vocab_entries(blocks, rules, textbook_id=str(BOOK_ID))\n",
    "linked_vocab = link_vocab_to_lessons(vocab_entries, lessons)\n",
    "\n",
    "# Filter for Unit 1\n",
    "unit_1_vocab = {v.term.lower() for v in linked_vocab if v.unit == UNIT_TO_TEST}\n",
    "print(f\"Found {len(unit_1_vocab)} vocabulary words for Unit {UNIT_TO_TEST}.\")\n",
    "if unit_1_vocab:\n",
    "    print(f\"Sample: {list(unit_1_vocab)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run RAG Pipeline\n",
    "We request a quiz generation. The RAG engine (`retrieve_and_generate`) searches the vector store for relevant content atoms. \n",
    "\n",
    "**Safeguard:** The `SearchService` (called internally) applies a `MetadataFilter` to restrict content to atoms with `sequence_index` <= Unit 1 (or matching the unit context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if should_mock:\n",
    "    print(\"Skipping LLM generation due to missing API key.\")\n",
    "else:\n",
    "    print(\"Generating Quiz...\")\n",
    "    try:\n",
    "        response = retrieve_and_generate(\n",
    "            book_id=str(BOOK_ID),\n",
    "            unit=UNIT_TO_TEST,\n",
    "            topic=TOPIC,\n",
    "            category=CATEGORY\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Generated Quiz ---\")\n",
    "        quiz_text = \"\"\n",
    "        for item in response.items:\n",
    "            q_str = f\"Q: {item.stem}\"\n",
    "            if item.options:\n",
    "                q_str += f\" Options: {item.options}\"\n",
    "            print(f\"{q_str} (Ans: {item.answer})\")\n",
    "            quiz_text += f\"{item.stem} {item.answer} \"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"RAG Execution Failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Safeguards\n",
    "We analyze the generated quiz text to see if it utilizes the vocabulary from Unit 1. While the LLM might use common English words (stop words), we specifically look for the presence of Unit 1 vocabulary terms to confirm relevant content was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not should_mock and 'quiz_text' in locals():\n",
    "    print(\"\\nVerifying Vocabulary Usage...\")\n",
    "    words_in_quiz = set(re.findall(r'\\b\\w+\\b', quiz_text.lower()))\n",
    "    \n",
    "    used_target_vocab = words_in_quiz.intersection(unit_1_vocab)\n",
    "    \n",
    "    print(f\"Total unique words in quiz: {len(words_in_quiz)}\")\n",
    "    print(f\"Unit 1 Vocab words used: {len(used_target_vocab)}\")\n",
    "    \n",
    "    if used_target_vocab:\n",
    "        print(f\"Examples used: {list(used_target_vocab)[:10]}\")\n",
    "        print(\"SUCCESS: The generated quiz incorporates specific vocabulary from Unit 1.\")\n",
    "    else:\n",
    "        if len(unit_1_vocab) > 0:\n",
    "            print(\"WARNING: No specific Unit 1 vocabulary detected. The quiz might be too generic.\")\n",
    "        else:\n",
    "            print(\"Note: No Unit 1 vocabulary was extracted to check against.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
