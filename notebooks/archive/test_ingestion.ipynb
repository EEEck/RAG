{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Ingestion Test\n",
    "\n",
    "This notebook demonstrates the parsing of a Docling JSON file, transformation into RAG primitives (`StructureNode`, `ContentAtom`), and insertion into a Postgres database with `pgvector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "\n",
    "# Add root to path so we can import 'ingest'\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from ingest.pipeline import run_ingestion\n",
    "from ingest.db import get_db_connection, ensure_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Database Schema\n",
    "# Ensure the DB is running (e.g., docker-compose up -d db)\n",
    "try:\n",
    "    conn = get_db_connection()\n",
    "    ensure_schema(conn)\n",
    "    print(\"Schema ensured.\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"Could not connect to DB: {e}\")\n",
    "    print(\"Make sure Postgres is running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run Ingestion for 'Green Line' (Real Data)\n",
    "book_id_1 = uuid.uuid4()\n",
    "json_path = \"../data/toy_green_line_1_docling.json\"\n",
    "\n",
    "if os.path.exists(json_path):\n",
    "    run_ingestion(json_path, book_id=book_id_1, should_mock_embedding=True)\n",
    "else:\n",
    "    print(f\"File not found: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Run Ingestion for 'Mock Book' (Test Partitioning)\n",
    "# We will reuse the same JSON but treat it as a different book ID to test partitioning logic.\n",
    "book_id_2 = uuid.uuid4()\n",
    "print(f\"Ingesting second book with ID: {book_id_2}\")\n",
    "\n",
    "if os.path.exists(json_path):\n",
    "    run_ingestion(json_path, book_id=book_id_2, should_mock_embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Verify Data in DB\n",
    "try:\n",
    "    conn = get_db_connection()\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # Check Structure Nodes\n",
    "    cur.execute(\"SELECT count(*) FROM structure_nodes\")\n",
    "    nodes_count = cur.fetchone()[0]\n",
    "    \n",
    "    # Check Content Atoms\n",
    "    cur.execute(\"SELECT count(*) FROM content_atoms\")\n",
    "    atoms_count = cur.fetchone()[0]\n",
    "    \n",
    "    print(f\"Total Structure Nodes: {nodes_count}\")\n",
    "    print(f\"Total Content Atoms: {atoms_count}\")\n",
    "    \n",
    "    # Check Partitioning (should see tables like content_book_...)\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT tablename FROM pg_tables \n",
    "        WHERE tablename LIKE 'content_book_%'\n",
    "    \"\"\")\n",
    "    partitions = cur.fetchall()\n",
    "    print(\"Partitions found:\", [p[0] for p in partitions])\n",
    "    \n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}