import pytest
import os
import uuid
import json
from pathlib import Path
from ingest.hybrid_ingestor import HybridIngestor
from ingest.models import ContentAtom, StructureNode
from ingest.schemas import LanguageMetadata, STEMMetadata, HistoryMetadata

# This file will be generated by the test
OUTPUT_FILE = Path(__file__).parent / "integration_rag.py"
DATA_FILE = Path("data/test_green_line_1.pdf")

def serialize_data(nodes, atoms):
    """
    Serializes nodes and atoms into a python file string.
    """
    # Helper to clean up string representation
    def clean_repr(obj):
        # We need to construct the imports for these objects to work
        return repr(obj)

    # We need to handle Pydantic models.
    # repr(atom) might show `ContentAtom(id=..., ...)` which is good if we import ContentAtom.
    # But for metadata fields which are Pydantic models, repr() works too.

    # Let's verify imports in the generated file
    lines = [
        "import uuid",
        "from uuid import UUID",
        "from ingest.models import ContentAtom, StructureNode",
        "from ingest.schemas import LanguageMetadata, STEMMetadata, HistoryMetadata",
        "",
        "# Generated Integration Data",
        f"DATA_NODES = {repr(nodes)}",
        "",
        f"DATA_ATOMS = {repr(atoms)}"
    ]
    return "\n".join(lines)

@pytest.fixture
def ingestor():
    return HybridIngestor()

def test_docling_ingestion_and_generation(ingestor):
    """
    Tests the default Docling ingestion path and generates the integration data file.
    """
    print("Test started. Verifying data file...")
    if not DATA_FILE.exists():
        pytest.fail(f"Data file {DATA_FILE} not found")

    book_id = uuid.uuid4()
    print(f"\nRunning Docling Ingestion on {DATA_FILE}...")

    # Docling is synchronous in hybrid_ingestor
    nodes, atoms = ingestor.ingest_book(str(DATA_FILE), book_id)

    assert len(nodes) > 0, "Docling failed to extract nodes"
    assert len(atoms) > 0, "Docling failed to extract atoms"

    print(f"Extracted {len(nodes)} nodes and {len(atoms)} atoms.")

    # Generate the file
    content = serialize_data(nodes, atoms)
    with open(OUTPUT_FILE, "w") as f:
        f.write(content)

    print(f"Generated integration data at {OUTPUT_FILE}")

def test_openai_ingestion(ingestor):
    """
    Tests the OpenAI VLM ingestion path.
    """
    # User requested hard fail if key is missing
    if not os.getenv("OPENAI_API_KEY"):
         pytest.fail("OPENAI_API_KEY not set in environment")

    if not DATA_FILE.exists():
        pytest.fail(f"Data file {DATA_FILE} not found")

    book_id = uuid.uuid4()
    print(f"\nRunning OpenAI Ingestion on {DATA_FILE}...")

    # HybridIngestor.ingest_with_openai is a synchronous wrapper calling asyncio.run internally.
    # We call it directly here.
    nodes, atoms = ingestor.ingest_with_openai(str(DATA_FILE), book_id, category="language")

    assert len(nodes) > 0
    assert len(atoms) > 0
    print(f"OpenAI Extracted {len(nodes)} nodes and {len(atoms)} atoms.")
